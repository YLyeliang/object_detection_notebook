\documentclass{article}

\usepackage[UTF8]{ctex}
\usepackage{amsmath}
%\usepackage{steinmetz}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\geometry{a4paper,scale=0.75}
% left=2cm,right=2cm,top=1cm,bottom=1cm
\title{目标检测Notebook}
\author{叶亮}
\date{\today}
\begin{document} 
\maketitle
\tableofcontents

\section{Base}

\subsection{基本知识点}
\textbf{离散型随机变量：}

期望： $E(X) = \sum_{i=1}^n{x_i p_i}$

方差：$D(X) = \sum_{i=1}^n{[x_i - E(x)]^2 p_i}$

\textbf{连续型随机变量：}

期望：$\int_{-\infty}^\infty{x f(x) dx}$

方差：$\int_{-\infty}^\infty{[x-E(x)]^2 f(x) dx}$

\subsection{distribution}
\subsubsection{伯努利分布}
Bernoulli distribution，又称\textbf{两点分布}或\textbf{0-1分布}，是一个离散型概率分布。记取值为1的概率为$p$，取值为0的概率为$q=1-p$,概率质量函数为
\begin{equation}
f(x;p) =  p^x(1-p)^{1-x} =  \begin{cases}
	p, \quad if\ x=1 \\
	q = 1 - p, \quad if\ x =0
	\end{cases}
\end{equation}
期望为
\begin{align}
E[X] = P(X=1)\cdot 1 + P(X=0) \cdot 0 = p  + 0 = p
\end{align}
方差
\begin{align}
Var(X) = \sum_{i=0}^1(x_i - E[X])^2 f(x) = (0 - p)^2(1-p) + (1-p^2)p = p(1-p) = pq
\end{align}

\subsubsection{二项分布}
\textbf{二项分布}	(Binomial distribution) 是n个独立的是/非试验中成功次数的离散概率分布，每次试验成功率为p。单次的试验为伯努利试验，即\textbf{n=1}时，二项分布为\textbf{伯努利分布}。

如果变量$X$服从参数为$n$和$p$的二项分布，则记为$X\sim b(n,p) or X\sim B(n,p)$。n次试验中正好得到k次成功的概率由概率质量函数给出：
\begin{align}
f(k,n,p) = Pr( X = k ) = \begin{pmatrix}
n\\ 
k
\end{pmatrix} p^k (1 - p)^{n-k}
\ where
\begin{pmatrix}
n\\ 
k
\end{pmatrix} = \frac{n!}{k!(n-k)!}
\end{align}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{images/distribution/binomial_distribution.png}
\caption{不同参数下的二项分布}
\label{Fig.Binomial_distribution}
\end{figure}

期望和方差为
\begin{align}
E(x) = np  \\
Var(x) = np( 1-p)
\end{align}

\subsection{cross-entropy}
熵是表示\textbf{随机变量不确定性的度量}.设$X$是一个取有限值的离散随机变量，其概率分布为
\begin{align}
P(X=x_i) = p_i, i =1,2,..., n
\end{align}
则随机变量$X$的熵定义为
\begin{align}
H(X) = - \sum_{i=1}^n p_i \log{p_i}
\end{align}
根据定义可知，熵的大小只依赖于X的分布，而与X的取值无关，所以可以将$X$的熵记为$H(p)$，即
\begin{align}
H(p) = - \sum_{i=1}^n p_i \log{p_i}
\end{align}
熵越大，则说明随机变量的不确定性越大，因此需要更多的信息去验证真假。
当X为二元变量时，例如1，0，则熵为
\begin{align}
H(p) = -p\log{p} - (1-p)\log{(1-p)}
\end{align}
当$p=0$或$p=1$时,$H(p)=0$，随机变量完全没有不确定性。当$p=0.5$时，$H(p) = 1$，熵取最大值，随机变量不确定性最大。

\subsection{神经网络相关知识}
\subsubsection{卷积相关}
参考论文:\href{https://arxiv.org/pdf/1603.07285.pdf}{A guide to convolution arithmetic for deep learning}.
\subsubsection{感受野}
\textbf{感受野}(Receptive field)，用来表示网络内部的不同位置的神经元对原图像的感受范围的大小。神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着他可能蕴含更为全局、语义层次更高的特征；而值越小则表示其所包含的特征越趋向于局部和细节。因此感受野的值可以大致用来判断每一层的抽象层次。参考博客：\href{https://blog.mlreview.com/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807}{a-guide-to-receptive-field}。

先介绍卷积的输出特征图尺寸计算公式
\begin{align}
n_{out} = \lfloor \frac{n_{in}+2p - k}{s} \rfloor + 1
\end{align}
其中，$n$为特征图尺寸大小，$k$为卷积核尺寸,$p$为padding尺寸，$s$为步长。

感受野的可视化方式有两种，如图\ref{Fig.receptive_field}所示，左边为常用的方式，右边为固定尺寸的可视化方式。右边通过固定尺寸的方式，可以更直观地将深层特征图的特征对应到之前特征图中的感受野中心位置。


感受野的计算公式一（常用方式）：
\begin{align}
RF_{l+1} = RF_l + (k_{l+1} -1) \times features\_stride_l
\end{align}
其中,$RF$为感受野大小，$l$表示层数，$features\_stride_l = \prod_{i=1}^l s_i$,$l=0$表示输入层，$RF_0=1$,$features\_stride_0=1$.

如果包含空洞卷积的话，则公式为
\begin{align}
RF_{l+1} = RF_l + (k_{l+1} -1) \times features\_stride_l \times dilation_{l+1}
\end{align}

感受野的计算公式二：
\begin{equation}
\begin{aligned}
& j_{out} = j_{in} * s \\
& r_{out} = r_{in} + (k -1) * j_{in} \\
& start_{out} = start_{in} + (\frac{k-1}{2} -p) * j_{in}
\end{aligned}
\end{equation}
其中，第一行公式计算输出特征图的jump大小，即每个特征之间的间距。第二行计算输出特征图的感受野尺寸。第三行计算第一个输出特征的感受野的中心位置，等于第一个输入特征的中心点加上第一个输入特征与第一个卷积中心的距离$\frac{k-1}{2}* j_{in}$，并减取padding空间$p*j_{in}$。

第一层为输入层，即图像本身,$n= image\ size, r=1,j=1, start=0.5$。图\ref{Fig.receptive_field_calc}所示为具体计算过程。

\begin{figure}
\centering
\includegraphics[scale=0.2]{images/receptive_field.png}
\caption{两种感受野的可视化方式}
\label{Fig.receptive_field}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.2]{images/receptive_field_calculation.png}
\caption{感受野的计算过程}
\label{Fig.receptive_field_calc}
\end{figure}


\section{Loss}
\subsection{IoU loss}
IoU Loss的基本计算公式为:
\begin{align}
\mathcal{L}_{IoU} = 1 - IoU
\end{align}
其中, IoU为预测和GT框的交并比，其他改进后的版本大多式在此基础上加入额外的惩罚项来有针对性地引导模型。其优点为:

1. 可以反映预测检测框与真实检测框的检测效果。

2. 尺度不变性，也就是对尺度不敏感（scale invariant）， 在regression任务中，判断predict box和gt的距离最直接的指标就是IoU。(满足非负性；同一性；对称性；三角不等性)。

缺点：

1. 如果两个框没有相交，根据定义，IoU=0，不能反映两者的距离大小（重合度）。同时因为loss=0，没有梯度回传，无法进行学习训练。

2. IoU无法精确的反映两者的重合度大小。如下图所示，三种情况IoU都相等，但看得出来他们的重合度是不一样的，左边的图回归的效果最好，右边的最差。
\begin{figure}[htp]
\centering
\includegraphics[scale=0.5]{images/IoU.jpg}
\caption{相同IoU的不同情况}
\label{Fig.IoU}
\end{figure}

\subsection{GIoU loss}
GIoU loss中的IoU部分替换成了GIoU,其计算方式如下:
\begin{align}
GIoU = IoU - \frac{\left| A_c - U \right|}{\left| A_c \right|}
\end{align}
其中，$A_c$表示两个框的最小闭包区域面积（同时包含预测和GT的最小框），然后计算闭包区域中不属于两个框的区域所占比重。特性：

1.与IoU相似，GIoU也是一种距离度量，作为损失函数的话， [公式] ,满足损失函数的基本要求；

2. GIoU对scale不敏感； 

3. GIoU是IoU的下界，在两个框无限重合的情况下，IoU=GIoU=1；

4. IoU取值[0,1]，但GIoU有对称区间，取值范围[-1,1]。在两者重合的时候取最大值1，在两者无交集且无限远的时候取最小值-1，因此GIoU是一个非常好的距离度量指标。

5. 与IoU只关注重叠区域不同，GIoU不仅关注重叠区域，还关注其他的非重合区域，能更好的反映两者的重合度。
\subsection{DIoU loss}
DIoU要比GIou更加符合目标框回归的机制，将目标与anchor之间的距离，重叠率以及尺度都考虑进去，使得目标框回归变得更加稳定，不会像IoU和GIoU一样出现训练过程中发散等问题。
\begin{align}
DIoU = IoU - 	\frac{\rho^2(b,b^{gt})}{c^2}
\end{align}
其中，$\rho()$为欧式距离计算，$b$为中心点坐标，$c$为最小闭包的对角线长度:$c^2= cw^2+ch^2$.
Code optimization, the calculation of distance between two center points of bboxes.
$C$包含$x$和$y$，以下公式为$x$计算部分，$y$部分同理，最后求和.
\begin{equation}
\begin{aligned}
(C_1-C_2)^2)&= ((x1_{C1}+x2_{C1})/2-(x1_{C2}+x2_{C2})/2)^2 \\
&=((x1_{C1}+x2_{C1})-(x1_{C2}+x2_{C2}))^2/4 \\
\end{aligned}
\end{equation}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.5]{images/DIoU.jpg}
\caption{DIoU:对anchor框和目标框之间的归一化距离进行了建模}
\label{Fig. DIoU}
\end{figure}
特性：

1. 与GIoU loss类似，DIoU loss($ \mathcal{L}_{DIoU}= 1- DIoU$)在与目标框不重叠时，仍然可以为边界框提供移动方向。

2. DIoU loss可以直接最小化两个目标框的距离，因此比GIoU loss收敛快得多。
3. 对于包含两个框在水平方向和垂直方向上这种情况，DIoU损失可以使回归非常快，而GIoU损失几乎退化为IoU损失。
4. DIoU还可以替换普通的IoU评价策略，应用于NMS中，使得NMS得到的结果更加合理和有效。

\subsection{CIoU loss}
论文考虑到bbox回归三要素中的长宽比还没被考虑到计算中，因此，进一步在DIoU的基础上提出了CIoU。其惩罚项如下面公式：
\begin{align}
\mathcal{R}_{CIoU} = \frac{\rho^2(b,b^{gt})}{c^2} + \alpha v
\end{align}
其中，$v$用来度量长宽比的相似性，定义为:$v=\frac{4}{\pi^2}(\arctan{\frac{w^{gt}}{h^{gt}}}- \arctan{\frac{w}{h}})^2 $， $\alpha$为权重函数，定义为:$\alpha = \frac{v}{(1-IoU)+v}$。最后，CIoU loss的梯度类似于DIoU loss，但还要考虑$v$的梯度。在长宽在 $[0,1]$ 的情况下，对$\arctan$进行求导时$w^2+h^2$的值通常很小，会导致梯度爆炸，因此在 $\frac{1}{w^2+h^2}$实现时将替换成1。在使用pytorch实现时，计算$\alpha$时使用 torch.no\_grad()来避免计算$\alpha$的损失，以此来保证合理的收敛过程。

\subsection{BIoU loss}

\section{Activation}
\subsection{Sigmoid}
Sigmoid函数，其公式如下:
\begin{align}
\sigma(x) = \frac{1}{1+e^{-x}} \\
\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))
\end{align}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{images/activation/sigmoid.jpg}
\caption{Sigmoid函数}
\label{Fig.sigmoid}
\end{figure}
在什么情况下适合使用 Sigmoid激活函数呢？

1. Sigmoid 函数的输出范围是 0 到 1。由于输出值限定在 0 到 1，因此它对每个神经元的输出进行了归一化；

2. 用于将预测概率作为输出的模型。由于概率的取值范围是 0 到 1，因此 Sigmoid 函数非常合适；梯度平滑，避免「跳跃」的输出值；

3. 函数是可微的。这意味着可以找到任意两个点的 sigmoid 曲线的斜率；

4. 明确的预测，即非常接近 1 或 0。

Sigmoid激活函数有哪些缺点？

1. 倾向于梯度消失；

2. 函数输出不是以 0 为中心的，这会降低权重更新的效率；

3. Sigmoid 函数执行指数运算，计算机运行得较慢。

\subsection{Tanh}
双曲正切函数，其图形也是S形，公式如下:
\begin{align}
tanh(x) = \frac{2}{1+e^{-2x}} - 1 = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}  \\
tanh'(x) = 1 - tanh^2(x)
\end{align}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{images/activation/tanh.jpg}
\caption{Tanh函数}
\label{Fig.tanh}
\end{figure}
相比sigmoid, tanh的优势体现在:

首先，当输入较大或较小时，输出几乎是平滑的并且梯度较小，这不利于权重更新。二者的区别在于输出间隔，tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；

tanh函数的缺点同sigmoid函数的第一个缺点一样，当x很大或很小时，导数接近于 0 ，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。

在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层.

\subsection{ReLU}
ReLU的公式如下:
\begin{equation}
relu(x) = \begin{cases}
	x,  x>=0 \\
	0, x < 0
\end{cases}
\end{equation}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{images/activation/relu.png}
\caption{ReLU函数}
\label{Fig.relu}
\end{figure}

ReLU6的公式如下，这是为了在移动端设备 float16/int8 的低精度的时候也能有很好的数值分辨率:
\begin{align*}
\text{ReLU6}(x) = \min(\max(0,x), 6)
\end{align*}

优点：
输入为正时，不存在梯度饱和问题；计算速度快。

缺点：

Dead ReLU 问题。当输入为负时，ReLU 完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，sigmoid 函数和 tanh 函数也具有相同的问题；

输出为 0 或正数，这意味着 ReLU 函数不是以 0 为中心的函数。

\subsection{Leaky ReLU \& Parametric ReLU}
\begin{equation}
LReLU(x) = \begin{cases}
	x, x>0 \\
	ax, x<=0
	\end{cases}
\end{equation}
\begin{figure}[htp]
\centering
\includegraphics[scale=1]{images/activation/elu.png}
\caption{ReLU、Leaky ReLU、ELU}
\label{Fig.elu}
\end{figure}
特性:

Leaky ReLU 通过把 x 的非常小的线性分量给予负输入（0.01x）来调整负值的零梯度（zero gradients）问题；如果$a$是可以学习的参数，则为\textbf{PReLU}(Parametric ReLU).

leak 有助于扩大 ReLU 函数的范围，通常 a 的值为 0.01 左右；

Leaky ReLU 的函数范围是（负无穷到正无穷）。

\subsection{ELU}
ELU 的提出也解决了 ReLU 的问题。与 ReLU 相比，ELU 有负值，这会使激活的平均值接近零。均值激活接近于零可以使学习更快，因为它们使梯度更接近自然梯度。
\begin{equation}
        \text{ELU}(x) = \begin{cases}
        x, & \text{ if } x > 0\\
        \alpha * (\exp(x) - 1), & \text{ if } x \leq 0
        \end{cases}
\end{equation}

ELU 具有 ReLU 的所有优点，并且：

1. 没有 Dead ReLU 问题，输出的平均值接近 0，以 0 为中心；

2. ELU 通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向零加速学习；

3. ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息。

一个小问题是它的计算强度更高。与 Leaky ReLU 类似，尽管理论上比 ReLU 要好，但目前在实践中没有充分的证据表明 ELU 总是比 ReLU 好。

\subsection{SELU}
SELU是在ELUde基础上引入了一个$\lambda$系数。当其中参数取为 $\lambda \approx 1.0507, \alpha \approx 1.6733$ 时，在网络权重服从标准正态分布的条件下，各层输出的分布会向标准正态分布靠拢。这种"自我标准化"的特性可以避免梯度消失和爆炸的问题。

论文提出时用于feed-forward neural network中，构成 Self-Normalizing Neural Networks(SNN).其主要作用在于将神经元的值归一化到0均值单位方差范围。以便于网络更好的收敛。

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{images/activation/selu.png}
\caption{SELU}
\label{SELU}
\end{figure}

\subsection{SiLU / Swish}
Sigmoid Linear Unit
\begin{align}
\text{silu}(x) = x * \sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}
\end{align}

\begin{figure}[htp]
\centering
\includegraphics[scale=1]{images/activation/SiLU.jpg}
\caption{The activation functions of the SiLU and the ReLU (left panel), and the dSiLU and the sigmoid unit (right panel)}
\label{SiLU}
\end{figure}

Swish 的设计受到了 LSTM 和高速网络中 gating 的 sigmoid 函数使用的启发。使用相同的 gating 值来简化 gating 机制，这称为 self-gating。self-gating 的优点在于它只需要简单的标量输入，而普通的 gating 则需要多个标量输入。这使得诸如 Swish 之类的 self-gated激活函数能够轻松替换以单个标量为输入的激活函数（例如 ReLU），而无需更改隐藏容量或参数数量。
Swish激活函数的主要优点如下：

「无界性」有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和；（同时，有界性也是有优势的，因为有界激活函数可以具有很强的正则化，并且较大的负输入问题也能解决);
平滑度在优化和泛化中起了重要作用。

\subsection{Hard swish}
\begin{equation}
        \text{Hardswish}(x) = \begin{cases}
            0 & \text{if~} x \le -3, \\
            x & \text{if~} x \ge +3, \\
            x \cdot (x + 3) /6 & \text{otherwise}
        \end{cases}
\end{equation}
主要是解决移动端下精度较低时的计算问题。
\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{images/activation/activation.jpg}
\caption{激活函数}
\end{figure}

\section{mmdetection}

\subsection{anchor}
mmdetection中，在retinanet, R-CNN等非ssd系列检测器的grid anchor生成中，在grid中每个anchor的坐标表示为(xmin,ymin,xmax,ymax)。anchor参数包含strides, ratios, scales, base sizes等. 其中strides为特征图的步长，ratios为宽高比，scales为anchor的缩放因子。计算过程如下:

1. 根据featmap生成grid.其中grid坐标为i * featmap size for i in grid size.

2. grid中每个cell根据 ratios,  scales和base sizes计算anchors,$xmin = grid_i -  basesize*scales*ratios, xmax = grid_i +basesize*scales*ratios$.
每个anchor的形式为(xmin,ymin,xmax,ymax),且均

 具体计算方式可参考\textbf{core/anchor/anchor\_generator.py}中部分。

\subsection{coder}
在RetinaNet、SSD、Cascade R-CNN等网络中，网络预测的bbox都会进行Delta xywh编码，即将原始的(xmin, ymin, xmax, ymax)进行编码,计算proposals相对GT的距离，得到(dx,dy,dw,dh)，来减小回归的过拟合，提升回归的稳定性。其中编码后的x为中心点。
\begin{equation}
\begin{aligned}
&\delta_x = (g_x-p_x)/p_w	   & \delta_y = (g_y - p_y)/p_h \\
&\delta_w = \log{\frac{g_w}{p_w}}	&\delta_h = \log{\frac{g_h}{p_h}}
\end{aligned}\label{bbox_coder}
\end{equation}

上述公式计算得到的$\delta$值通常很小，因为网络通常只对p进行少量微调，导致回归loss比分类loss小很多。为了提升学习的有效性，$\delta$通常需要经过均值和方差进行标准化.

\begin{equation}
\begin{aligned}
\delta_{x}^{'}=\frac{\delta_x-\mu_x}{\rho_x}
\end{aligned}
\end{equation}

\subsection{two-stage}
mmdetection中的二阶段检测网络，首先使用RPN-FPN网络，得到多个level的特征图，然后使用多个level的特征图生成proposals,并使用NMS进行过滤。然后利用过滤后的proposals,在对应特征图上进行ROIAlign和ROIPool.在RPN阶段，计算loss时分类loss只计算class-agnostic loss，即采样到的gt targets为正的框。
\subsubsection{ROI Pool}
ROI pool根据生成的proposals,(xyxy),在对应的feature maps上进行裁剪，将对应坐标内的特征图分成$s*s$块状网格，每个网络里面可能由多个值，然后使用最大池化对每个网络内的多个值进行池化，得到$s*s$的尺寸输出。在对特征图进行分块时，ROI pool使用直接取整的方式，以一个$8*8$的特征图为例，希望输出$2*2$的池化后的特征图。假设(xyxy)为(0,3,7,8),则proposal的原始h,w为5,7. 做不到等分成4块。ROI pool的处理流程为：1)将h分成两部分，5/2 = 2.5 --> 2(向下取整), 剩下的为3，则高分成两块，一块包含两个特征点，一块包含三个特征点; 2) 7/2 = 3.5 -->3， 剩下的为4，则宽分成两块，一块包含3个特征点，一块包含4个特征点。
\begin{figure}
\centering
\includegraphics[scale=0.5]{images/roipool.jpg}
\caption{ROI-Pool过程}
\label{Fig.roi_pool}
\end{figure}

\section{training}
\subsection{Optimizer}
\subsubsection{warmup}
warmup通常有三个方式:linear, constant, exp. 通常需要设置warmup的迭代数$iter_{total}$和warmup的增加比率ratio,
\begin{align}
lr_t = lr_{constant}*ratio
\end{align}
\begin{equation}
\begin{aligned}
lr_t = lr_{const}*(1-k), \\
k= (1-iter_t /iter_{total}) * (1-ratio)
\end{aligned}
\end{equation}
\begin{align}
lr_t = lr_{const}*k, \\
k=ratio^{1-iter_t/iter_{total}}
\end{align}

\section{Yolov4}
Yolov4的实现过程中,anchor的生成以及label与anchor的对应关系的构建方法。如图\ref{Fig.yolo_bbox}.所示。
\subsection{Anchor generation \& target build}
与retinanet，rcnn系列等bbox预测不同的是，yolo模型推理得到的结果（x,y,w,h)需要经过如下公式进行编码：
\begin{equation}
\begin{aligned}
b_x = \sigma(t_x)+c_x \\
b_y = \sigma(t_y)+c_y \\
b_w = p_we^{t_w} \\
b_h = p_he^{t_h}
\end{aligned}
\end{equation}
\begin{figure}[htp!]
\centering
\includegraphics[scale=0.3]{images/yolo_bbox.png}
\caption{Yolo bbox prediction}
\label{Fig.yolo_bbox}
\end{figure}

\section{Yolov5}
\subsection{Model}
\textbf{Focus结构}: Focus模块中，首先将一幅图，按照隔点采样(::2,1::2)的方式，将一个图片分成四幅图，然后将这四幅图拼接到一块，形成$3*4=12$通道的特征图，再做卷积操作，这样来实现下采样和卷积操作。

\textbf{C3}: CSP bottleneck with 3 convolution\cite{wang2020cspnet}.其中CSP模块如图\ref{Fig.csp}所示，采用fusion first方式，通过1x1的卷积将base layer分成均等的两部分特征图，其中part 1 和part2为hidden channels,为最终输出通道的1/2.C3中的bottleneck为标注的残差模块，使用1x1-3x3和shortcut实现。

\textbf{Conv}:conv模块中包含了conv-bn-act三部分，其中act使用SiLU激活函数，在特征提取的下采样过程中，没有使用maxpool，而是采用了s=2的conv来实现。

\textbf{head}:yolov5检测头与yolov3, 4系列一样，每一层输出的通道为(num classes + 5(xywh,conf) ) * num anchors, e.g 80类别，三个anchor, 则输出通道为 (80+5)*3 = 255. 对最后的预测特征图使用sigmoid归一化到0-1区间.

\begin{figure}[htp!]
\centering
\includegraphics[scale=0.6]{images/csp_block.png}
\caption{CSP模块}
\label{Fig.csp}
\end{figure}

\subsection{Loss}


\subsection{Inference}
在yolov5的推理部分中，预测pred的通道为xywh,obj\_conf, nc。首先，使用conf\_thres 对 obj\_conf 进行过滤，保留一定数目的bbox,然后对所有的类别概率进行重计算$nc\_conf = obj\_conf * nc\_conf$,

\section{backbone}
\subsection{Regularization}
\subsubsection{Dropout}
原理,dropout随机丢弃神经元(全连接中输入神经元)，实现方式为$keep_prob$,每个神经元生成一个随机数$k,k<keep_prob$即丢弃。优点,该方法有利于分类中泛化能力的提升.
\subsubsection{Drop Connect}
\subsubsection{Drop block}


\section{Refinedet}
\subsection{Anchor}
Refine中的anchor计算。对于每一个feature map, 首先计算其mesh grid,然后计算每个框的中心点$(x,y)=(\frac{i+0.5}{feat\_size},\frac{j+0.5}{feat\_size})$,然后根据每个feature map对应的anchor box的大小，计算anchor的长和宽$WH_{ki}=\frac{box_k}{image\_size}$, k表示第k层特征图,i表示第i个网格. e.g, 使用四个feature level, $box=[32,64,128,256], image\_size=320, feat\_size=[40,20,10,5], aspect\_ ratio=[2,2,2,2]$, 那么最终生成$40*40*3+20*20*3+10*10*3+5*5*3=6375$个anchor.

\subsection{Loss}
计算过程分为arm和odm，其中arm预测分别输出$num\_anchor*4$和$num\_anchor*2$通道的特征图(坐标，是否包含目标)；odm预测分别输出$num\_anchor*4$和$num\_anchor*num\_classes$通道的特征图(坐标，类别数量)。每层特征图的目标数量则为$N_i*H_i*num\_anchor$。

在loss计算过程中，分别计算分类loss和回归loss.其中分类使用交叉熵，回归使用smooth l1



\section{Dataset}
\subsection{COCO Dataset}
COCO dataset 全称 Common Objects in Context. 共有80个类。共有三种标注类型：object instance, object keypoints, image captions。使用json文件进行存储。
\subsubsection{object instance}
{
    "info": info,
    "licenses": [license],
    "images": [image],
    "annotations": [annotation],
    "categories": [category]
}

\section{Detector}
\subsection{CornerNet, CenterNet}
\subsubsection{targets computation}
在anchor free网络中，生成gt bbox的heatmap target时，相应的高斯半径的计算方式如下。一共存在三种情况：
1，生成的target与gt bbox有重叠部分，其中一个corner在gt box内部，另一个在gt box外部。
\begin{gather}
        \cfrac{(w-r)*(h-r)}{w*h+(w+h)r-r^2} \ge {iou} \quad\Rightarrow\quad
        {r^2-(w+h)r+\cfrac{1-iou}{1+iou}*w*h} \ge 0 \\
        {a} = 1,\quad{b} = {-(w+h)},\quad{c} = {\cfrac{1-iou}{1+iou}*w*h} \\
        {r} \le \cfrac{-b-\sqrt{b^2-4*a*c}}{2*a}
\end{gather}
2.两个corner都在gt box里面
\begin{gather}
\cfrac{(w-2*r)*(h-2*r)}{w*h} \ge {iou} \quad\Rightarrow\quad
        {4r^2-2(w+h)r+(1-iou)*w*h} \ge 0 \\
        {a} = 4,\quad {b} = {-2(w+h)},\quad {c} = {(1-iou)*w*h} \\
        {r} \le \cfrac{-b-\sqrt{b^2-4*a*c}}{2*a}
\end{gather}
3.两个corner都在gt box外部
\begin{gather}
\cfrac{w*h}{(w+2*r)*(h+2*r)} \ge {iou} \quad\Rightarrow\quad
        {4*iou*r^2+2*iou*(w+h)r+(iou-1)*w*h} \le 0 \\
        {a} = {4*iou},\quad {b} = {2*iou*(w+h)},\quad {c} = {(iou-1)*w*h} \\
        {r} \le \cfrac{-b+\sqrt{b^2-4*a*c}}{2*a}
\end{gather}
高斯核的计算
\begin{align}
\exp^{\cfrac{-(x*x+y*y)}{2*\sigma*\sigma}}
\end{align}

\subsubsection{decode heatmap}
对heatmap进行解码，遵循如下顺序:1. nms on heatmap. 2. get topk positions from heatmap. 3. decode offset and wh size.

\section{Optical Character Recognition}
\subsection{DBNet}
Real-Time Scene Text Detection with Differentiable Binarization\cite{liao2020real}.
\subsubsection{Related work}
最近的OCR可分为两种：Regression-based and segmentation-based.

\textbf{Regression-based methods} 直接回归文本实例的坐标框,使用NMS后处理。大部分模型达不到精准的文本框检测（非矩形，非旋转矩形等），尤其是弯曲的文本位置。

\textbf{Segmentation-based methods} 结合Pixel-level prediction and post-processing 来获取文本位置。
\begin{figure}[htp!]
\centering
\includegraphics[scale=0.4]{images/dbnet.png}
\caption{Architecture of the DB-Net}
\label{Fig.dbnet}
\end{figure}

\subsubsection{Methodology}
如图Fig. \ref{Fig.dbnet}所示，使用FPN作为backbone,在1/4阶段做预测，其中"pred"由一个3x3卷积和两个反卷积组成。生成概率图($P$)和阈值图($T$)，并通过P和T来计算approximate binary map($\hat{B}$).二值化过程可表述为如下公式：
\begin{equation}
B_{i,j} = \left\{
\begin{array}{lr}
1 & if \: P_{i,j} >=t, \\
0 & otherwise.
\end{array}
\right.
\label{Eq.binarization}
\end{equation}
\textbf{Differentiable binarization} 公式\ref{Eq.binarization}不可微。所以在训练时不能通过网络对其进行优化，因此论文提出如下近似step function:
\begin{align}
\hat{B}_{i,j}=\dfrac{1}{1+e^{-k(P_{i,j}-T_{i,j})}}
\end{align}
其中,$k$表示增强因子，设置成50.DB提升性能可归结于梯度方向传播。以二值交叉熵为例，定义$f(x)=\dfrac{1}{1+e^{-kx}}$为DB函数，其中$x=P_{i,j}-T_{i,j}$,正类标签的loss $l_+$和负类标签的loss $l_-$分别为：
\begin{equation}
\begin{aligned}
l_+ = -\log{\dfrac{1}{1+e^{-kx}}} \\
l_- = -\log{(1-\dfrac{1}{1+e^{-kx}})}
\end{aligned}
\end{equation}
使用链式规则可以得到如下微分：
\begin{align}
\dfrac{\partial l_+}{\partial x} = -kf(x)e^{-kx} \\
\dfrac{\partial l_-}{\partial x} = kf(x)
\end{align}

\textbf{Label Generation}
PSENet生成方法：给定文本图像，每个文本区域的多边形由一系列线段进行描述：
\begin{align}
G = \{ S_k \}_{k=1}^n
\end{align}
其中，$n$表示顶点数量，在不同的数据集中不一样。ICDAR 2015为4，CTW1500为16.然后使用Vatti clipping算法将多边形$G$收缩程$G_s$得到正类区域.其中，收缩的偏移量D由原多边形的周长L和面积A计算得到：
\begin{align}
D=\dfrac{A(1-r^2)}{L}
\end{align}
$r$为收缩比例，一般设置为0.4. 通过类似方法，为阈值图生成标签。1. 多边形$G$通过相同的偏移$D$进行膨胀得到$G_d$.2. 将$G_s$和$G_d$之间的间隙(gap)作为文本区域的边界，其中阈值图的标签通过计算距离$G$中最近的线段距离得到。

\textbf{Optimization}

Loss表示为概率图$L_s$, 二值图$ L_b $和阈值图$L_t$的加权和:
\begin{align}
L=L_s + \alpha \times L_b + \beta \times L_t
\end{align}
$\alpha$和$\beta$分别设置为1.0和10。对$L_s$和$L_b$应用binary cross-entropy(BCE)loss，使用hard negative mining来缓解正负样本的非平衡问题：
\begin{align}
L_s = L_b = \sum_{i \in S_l}{y_i \log{x_i} + (1 - y_i) \log{1 - x_i}} 
\end{align}
其中,$S_l$为采样集，正负样本比例为1:3.

$L_t$使用$L_1$距离和来计算loss，为膨胀后的文本多边形区域$G_d$内预测和标签的距离：
\begin{align}
L_t = \sum_{i \in R_d}{\left| y_i^* - x_i^* \right|}
\end{align}
其中，$R_d$为膨胀后的多边形$G_d$内的像素索引集合，$y_i^*$为阈值图标签

在推理阶段，可以只用概率图或近似二值图来生成文本坐标框，其结果相似，任选一即可。论文中，为了更好的效率，使用概率图来生成文本框，这样可以移除阈值图。即，box处理过程为三个步骤：1）从概率图/近似二值图通过固定阈值(0.2)来首次二值化得到二值化图； 2）从二值图得到连接区域（收缩的文本区域）； 3）使用偏移量$D'$， Vatti clipping算法来膨胀收缩区域。$D'$计算方式为：
\begin{align}
D' = \dfrac{A' \times r'}{L'}
\end{align}
其中，$A'$为收缩多边形的面积；$L'$为收缩多边形的周长；$r'$通过经验设置为$1.5$.

\subsubsection{Implementation Details}
数据集介绍： \textbf{SynthText}，\textbf{MLT-2017 dataset}，\textbf{ICDAR 2015 dataset}，\textbf{MSRA-TD500 dataset}，\textbf{CTW1500 dataset}，\textbf{Total-Text dataset}.

训练时，使用SynthText预训练100k iteration, 然后在真实样本上微调1200epochs。batch size 16,使用余弦学习率下降策略。其中当前迭代的学习率为$lr_{init} \times (1 - \dfrac{iter}{max\_iter})^{power}$.初始学习率为$0.007$,$power$为0.9.weight decay of 0.0001, momentum of 0.9.

数据增强：1）随机旋转，角度区间$(-10^\circ,10^\circ)$；2）随机裁剪；3）随机翻转。所有处理图片均resize到640x640。

\subsection{CRNN}
文本识别模型(Text recognition model). 包含特征提取，序列建模，统一框架转录。有四个特性：1）端到端；2）处理任意长度的序列；3）不受预定义词典限制；4）公开数据上取得较好效果；

贡献：提出了CRNN网络（Convolutional Recurrent neural network）.1)可直接学习标签序列；2）直接从图像数据中学习有用信息，不需要手工特征和其他预处理；3）有RNN的特性，可输出标签序列；4）不受序列目标长度限制；5）效果较好；6）参数较少

相关流程解析资源：\href {https://wandb.ai/authors/text-recognition-crnn-ctc/reports/Text-Recognition-With-CRNN-CTC-Network--VmlldzoxNTI5NDI}{wandb-text-recognition-crnn-ctc}

\begin{figure}
\centering
\includegraphics[scale=0.5]{images/CRNN/CRNN.png}
\caption{CRNN结构}
\label{Fig.crnn}
\end{figure}

\subsubsection{Architecture}
结构如图\ref{Fig.crnn}所示.从下往上依次为：卷积层、循环层和转录层。conv layer用于提取特征序列，recurrent layer用于对特征序列的每一帧做预测；transcription layer则将每帧预测翻译为标签序列；可以使用单个loss端到端训练。

\textbf{Feature Sequence Extraction}

使用标准backbone进行特征提取。所有输入图像须保持同一高度，由CNN生成特征向量序列，每个特征向量的特征序列从左至右在特征图上按列生成（即，第i个特征向量为特征图的第i列特征的拼接结果），每一列的宽度为一个像素。特征序列如图\ref{Fig.crnn_feature}所示。

\begin{figure}
\centering
\includegraphics[scale=0.5]{images/CRNN/CRNN_feature_extract.png}
\caption{特征序列}
\label{Fig.crnn_feature}
\end{figure}

\textbf{Sequence Labeing}

a deep bidirectional Recurrent Neural Network is built.对于特征序列$\textbf{x}=x_1,...,x_T$,recurrent layer为特征序列的每一帧$x_t$预测一个标签分布$y_t$.RNN的优点有三：1）RNN能有效发挥捕获上下文信息的能力。如图\ref{Fig.crnn_feature}所示，宽的字符需要连续的几帧来做一个完全的描述。2）RNN能将错误部分的导数反向传播给输入；3）RNN能处理任意长度的序列。

LSTM，包含一个cell和三个们，输入、输出和遗忘门。与一般LSTM不同，基于图像的序列，前向和后向的上下文信息都有用且互补。因此，提出采用forward和backward lstm的方式

\begin{figure}
\centering
\includegraphics[scale=0.5]{images/CRNN/lstm_unit.png}
\caption{bidirectional LSTM}
\label{Fig.bidirectional_lstm}
\end{figure}

\textbf{Transcription}

转录是将由RNN生成的每帧预测转换成标签序列的过程。数学上来将，即在每帧预测条件下，找到最大概率的标签序列。实际使用中，有两种方式：lexicon-free 和lexicon-based,无词汇的和基于词汇的。

\textbf{1. Probability of label sequence}

采用Connectionist Temporal Classification(CTC) layer中定义的条件概率。在每帧预测$\textbf{y}=y_1,....,y_T$条件下，概率为标签序列$\textbf{l}$定义，且忽略了$\textbf{l}$中每个元素的位置。因此，在使用负对数概率作为目标函数来训练时，可以只需要图像和对应的标签序列，省去了人工标注每一个符号位置的麻烦。

公式描述如下：输入序列$\textbf{y}=y_1,...y_T$,$T$为序列长度。$y_t \in \mathcal{R}^{|\mathcal{L}'|}$为集合$\mathcal{L}'=\mathcal{L}\cup$的概率分布，其中$\mathcal{L}$包含任务中的所有标签（e.g. all English characters）。一个sequence-to-sequence映射函数$\mathcal{B}$在序列$\boldsymbol{\pi} \in \mathcal{L}'^T$上定义，其中$T$为长度。$\mathcal{B}$将$\boldsymbol{\pi}_i$映射到$\textbf{l}$,通过首先移除重复的标签，然后移除'blank'（空）。比如，$\mathcal{B}$将"--hh-e-l-ll-oo--"(-表示blank)映射为"hello". 则，条件概率定义为由$\mathcal{B}$映射为$\textbf{l}$的所有$\boldsymbol{\pi}$的概率之和:
\begin{align}
p(\boldsymbol{l}|\textbf{y}) = \sum_{\boldsymbol{\pi}:\mathcal{B}(\boldsymbol{\pi})=\boldsymbol(l)} p(\boldsymbol{\pi}|\textbf{y}) 
\end{align}
其中，$p(\boldsymbol{\pi}|\textbf{y} = \prod_{t=1}^T y_{\pi_t}^t$,$y_{\pi_t}^t$为t时刻拥有标签$\pi_t$的概率。

\textbf{2. Lexicon-free transcription}

在这种情况下，拥有最大概率的$\textbf{l}^*$将作为预测。序列近似为$\textbf{l}^* \approx \mathcal{B}(\text{arg max}_\pi p(\boldsymbol{\pi}|\textbf{y}$.即，在每个时间$t$取最大概率的标签$\pi_t$,并将结果映射到$\textbf{l}^*$

\textbf{3. Lexicon-based transcription}

该模式下，每一个测试样本会绑定一个词库$\mathcal{D}$.标签序列通过选择词典中拥有最大条件概率的序列为作为识别结果。$\textbf{l}^* = arg max_{\textbf{l} \in \mathcal{D}} p(\textbf{l}|\textbf{y})$。
但是，对于大型词库，进行穷举搜索会非常耗时。对于该问题，注意到在使用lexicon-free时其预测的标签序列与GT的编辑距离相近。因此，可以限制搜索范围在一个最近邻候选区域$\mathcal{N}_\delta(\textbf{l}')$,其中$\delta$为最大编辑距离，$\textbf{l}'$为lexicon-free下转录的序列:
\begin{align}
\textbf{l}^* = \text{arg max}_{\textbf{l} \in \mathcal{N}_\delta(\textbf{l}')} p(\textbf{l}| \textbf{y)}
\end{align}

候选区域$\mathcal{N}_\delta(\textbf{l}')$可使用BK树数据结构来有效寻找。其时间复杂度为$O(log|\mathcal{D}|$,其中$|\mathcal{D}|$为词库大小。

\textbf{Network Training}

将训练集表示为$\mathcal{X}={I_i,\textbf{l}_i}_i$,其中$I_i$为图像，$\textbf{l}_i$为标签序列。目标函数为:
\begin{align}
\mathcal{O} = - \sum_{I_i,\textbf{l}_i \in \mathcal{X}} \log p(\textbf{l}_i| \textbf{y}_i)
\end{align}
使用SGD来训练。在recurrent layer,使用Back-Propagation Through Time(BPTT)来计算误差微分。

相关参数如图\ref{Fig.crnn_table}所示.
\begin{figure}
\centering
\includegraphics[scale=0.5]{images/CRNN/crnn_table.png}
\caption{模型参数}
\label{Fig.crnn_table}
\end{figure}

\subsection{Why you should try the Real Data for the STR}
Why you should try the Real Data for the Scene Text Recognition.

OCR用途：数字化之前的实体文本，帮助盲人阅读，车牌识别等。

文本识别模型分为4步：transformation, feature extraction, sequence modelling and prediction. 本文使用TPS for transofrmation, ResNeXt作特征提取，convolutional encoder-recurrent decoder from YAMTS with 2D attention作序列建模和预测。

\subsubsection{Architecture}
使用了ASTER \cite{shi2018aster}的图像矫正方法(rectification)	,目前该模块已成为标注模块。使用ResNext作特征提取，使用 \cite{krylov2021open}的文本检测头，具体结构如图 \ref{Fig. yet_arch}所示
\begin{figure}
\centering
\includegraphics[scale=0.5]{images/Yet_Another/Architecture.png}
\caption{网络结构}
\label{Fig. yet_arch}
\end{figure}


\bibliographystyle{IEEEtran}
\bibliography{reference.bib}


\end{document}